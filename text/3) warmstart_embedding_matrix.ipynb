{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to run this on\n",
    "# conda activate tf211\n",
    "\n",
    "# Run Date: Thursday, January 12, 2023\n",
    "# Run Time: 00:01:06\n",
    "\n",
    "# Run Date: Wednesday, January 11, 2023\n",
    "# Run Time: : 00:00:50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloaded from ...\n",
    "# https://www.tensorflow.org/tutorials/text/warmstart_embedding_matrix\n",
    "# on ...\n",
    "# Saturday, December 17, 2022\n",
    "\n",
    "# docker run --gpus all -it -v $(realpath ~/Data):/tf/Data -p 8888:8888 -p 6006:6006 tf211:20221217\n",
    "\n",
    "# Nice! I all runs! Tensorboard stuff does not load, but meh!\n",
    "# Run Date: Saturday, December 17, 2022\n",
    "# Run Time: : 00:00:50\n",
    "\n",
    "import time\n",
    "from datetime import date\n",
    "\n",
    "startTime = time.time()\n",
    "todaysDate = date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YS3NA-i6nAFC"
   },
   "outputs": [],
   "source": [
    "##### Copyright 2022 The TensorFlow Authors.\n",
    "\n",
    "\n",
    "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SN5USFEIIK3"
   },
   "source": [
    "# Warm-start embedding layer matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aojnnc7sXrab"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/warmstart_embedding_matrix\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/warmstart_embedding_matrix.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/warmstart_embedding_matrix.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/warmstart_embedding_matrix.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6mJg1g3apaz"
   },
   "source": [
    "This tutorial shows how to \"warm-start\" training using the [`tf.keras.utils.warmstart_embedding_matrix`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/warmstart_embedding_matrix) API for text sentiment classification when changing vocabulary.\n",
    "\n",
    "You will begin by training a simple Keras model with a base vocabulary, and then, after updating the vocabulary, continue training the model. This is referred to as \"warm-start\" training, for which you'll need to remap the text-embedding matrix for the new vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZhifmcDwJTf"
   },
   "source": [
    "## Embedding matrix\n",
    "\n",
    "Embeddings provide a way to use an efficient, dense representation in which similar vocabulary tokens have a similar encoding. They are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to have embeddings that are 8-dimensional for small datasets, and up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but can take more data to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2voNac7BwJ-g"
   },
   "source": [
    "### Vocabulary\n",
    "\n",
    "The set of unique words is referred to as the vocabulary. To build a text model you need to choose a fixed vocabulary. Typically you you build the vocabulary from the most common words in a dataset. The vocabulary allows us to represent each piece of text by a sequence of ID's that you can lookup in the embedding matrix. Vocabulary allows us to represent each piece of text by the specific words that appear in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuBkjGwtwKiv"
   },
   "source": [
    "### Why warm-start an embedding matrix?\n",
    "\n",
    "A model is trained with a set of embeddings that represents a given vocabulary. If the model needs to be updated or improved you can train to convergence significantly faster by reusing weights from a previous run. Using the embedding matrix from a previous run is more difficult. The problem is that any change to the vocabulary invalidates the word to id mapping.\n",
    "\n",
    "The `tf.keras.utils.warmstart_embedding_matrix` solves this problem by creating an embedding matrix for a new vocabulary from an embedding martix from a base vocabulary. Where a word exists in both vocabularies the base embedding vector is copied into the correct location in the new embedding matrix. This allows you to warm-start training after any change in the size or order of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZUQErGewZxE"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BfPukisbG_Yu"
   },
   "outputs": [],
   "source": [
    "# !pip install --pre -U \"tensorflow>2.10\"  # Requires 2.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RutaI-Tpev3T",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 18:16:06.576277: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-12 18:16:07.261868: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64\n",
      "2023-01-12 18:16:07.261960: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64\n",
      "2023-01-12 18:16:07.261967: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBFctV8-JZOc"
   },
   "source": [
    "### Load the dataset\n",
    "The tutorial uses the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/). You will train a sentiment classifier model on this dataset and in the process learn embeddings from scratch. Refer to [Loading text tutorial](https://www.tensorflow.org/tutorials/load_data/text) to learn more.  \n",
    "\n",
    "Download the dataset using Keras file utility and review the directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataDirectory. Notice it is outside this repo.\n",
    "dataDir = '/home/rob/Data/Documents/Github/rkaunismaa/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aPO4_UmfF0KH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "# dataset = tf.keras.utils.get_file(\n",
    "#     \"aclImdb_v1.tar.gz\", url, untar=True, cache_dir=\".\", cache_subdir=\"\"\n",
    "# )\n",
    "dataset = tf.keras.utils.get_file(\n",
    "    \"aclImdb_v1.tar.gz\", url, untar=True, cache_dir=dataDir, cache_subdir=\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['imdbEr.txt', 'imdb.vocab', 'test', 'train', 'README']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = os.path.join(os.path.dirname(dataset), \"aclImdb\")\n",
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eY6yROZNKvbd"
   },
   "source": [
    "The `train/` directory has `pos` and `neg` folders with movie reviews labelled as positive and negative respectively. You will use reviews from `pos` and `neg` folders to train a binary classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9-iOHJGN6SDu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['urls_pos.txt',\n",
       " 'urls_unsup.txt',\n",
       " 'neg',\n",
       " 'labeledBow.feat',\n",
       " 'unsup',\n",
       " 'unsupBow.feat',\n",
       " 'pos',\n",
       " 'urls_neg.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = os.path.join(dataset_dir, \"train\")\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9O59BdioK8jY"
   },
   "source": [
    "The `train` directory also contains additional folders which should be removed before creating the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1_Vfi9oWMSh-"
   },
   "outputs": [],
   "source": [
    "remove_dir = os.path.join(train_dir, \"unsup\")\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFoJjiEyJz9u"
   },
   "source": [
    "Next, create a `tf.data.Dataset` using `tf.keras.utils.text_dataset_from_directory`. You can read more about using this utility in this [text classification tutorial](https://www.tensorflow.org/tutorials/keras/text_classification). \n",
    "\n",
    "Use the `train` directory to create the training and validation sets with a split of 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ItYD3TLkCOP1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 18:16:21.213765: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-12 18:16:21.213988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-12 18:16:21.238564: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-12 18:16:21.238813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-12 18:16:21.238963: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-12 18:16:21.239098: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-12 18:16:21.239229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2006] Ignoring visible gpu device (device: 1, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1) with core count: 5. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\n",
      "2023-01-12 18:16:21.240111: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-12 18:16:21.240509: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-12 18:16:21.240731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-12 18:16:21.240868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-12 18:16:21.885903: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-12 18:16:21.886151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-12 18:16:21.886302: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-12 18:16:21.886427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5299 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "seed = 123\n",
    "# train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "#     \"aclImdb/train\",\n",
    "#     batch_size=batch_size,\n",
    "#     validation_split=0.2,\n",
    "#     subset=\"training\",\n",
    "#     seed=seed,\n",
    "# )\n",
    "# val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "#     \"aclImdb/train\",\n",
    "#     batch_size=batch_size,\n",
    "#     validation_split=0.2,\n",
    "#     subset=\"validation\",\n",
    "#     seed=seed,\n",
    "# )\n",
    "\n",
    "train_dir = dataset_dir + '/train'\n",
    "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=seed,\n",
    ")\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHV2pchDhzDn"
   },
   "source": [
    "### Configure the dataset for performance\n",
    "\n",
    "You can learn more about `Dataset.cache` and `Dataset.prefetch`, as well as how to cache data to disk in the [data performance guide](https://www.tensorflow.org/guide/data_performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Oz6k1IW7h1TO"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGicgV5qT0wh"
   },
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6NZSqIIoU0Y"
   },
   "source": [
    "Next, define the dataset preprocessing steps required for your sentiment classification model. Initialize a `layers.TextVectorization` layer with the desired parameters to vectorize movie reviews. You can learn more about using this layer in the [Text Classification](https://www.tensorflow.org/tutorials/keras/text_classification) tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2MlsXzo-ZlfK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rob/anaconda3/envs/tf211/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 10000\n",
    "sequence_length = 100\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "# Make a text-only dataset (no labels) and call `Dataset.adapt` to build the\n",
    "# vocabulary.\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zI9_wLIiWO8Z"
   },
   "source": [
    "## Create a classification model\n",
    "\n",
    "Use the [Keras Sequential API](https://www.tensorflow.org/guide/keras/sequential_model) to define the sentiment classification model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pHLcFtn5Wsqj"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "text_model_input = tf.keras.layers.Input(dtype=tf.string, shape=(1,))\n",
    "text_embedding = Embedding(vocab_size, embedding_dim, name=\"embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "iXAfZyEIRVY5"
   },
   "outputs": [],
   "source": [
    "text_input = tf.keras.Sequential(\n",
    "    [vectorize_layer, text_embedding], name=\"text_input\"\n",
    ")\n",
    "classifier_head = tf.keras.Sequential(\n",
    "    [GlobalAveragePooling1D(), Dense(16, activation=\"relu\"), Dense(1)],\n",
    "    name=\"classifier_head\",\n",
    ")\n",
    "\n",
    "model = tf.keras.Sequential([text_input, classifier_head])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjLNgKO7W2fe"
   },
   "source": [
    "## Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  validation\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rob/Data/Documents/Github/rkaunismaa/logs/TensorFlowTutorials/text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpX9etB6IOQd"
   },
   "source": [
    "You will use [TensorBoard](https://www.tensorflow.org/tensorboard) to visualize metrics including loss and accuracy. Create a `tf.keras.callbacks.TensorBoard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "W4Hg3IHFt4Px"
   },
   "outputs": [],
   "source": [
    "logs_dir = '/home/rob/Data/Documents/Github/rkaunismaa/logs/TensorFlowTutorials/text'\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OrKAKAKIbuH"
   },
   "source": [
    "Compile and train the model using the `Adam` optimizer and `BinaryCrossentropy` loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "lCUgdP69Wzix"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "5mQehiQyv8rP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-12 18:16:26.985299: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7fe4840a6d90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-01-12 18:16:26.985323: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 2070 SUPER, Compute Capability 7.5\n",
      "2023-01-12 18:16:26.988747: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-01-12 18:16:27.079613: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 6s 214ms/step - loss: 0.6923 - accuracy: 0.5028 - val_loss: 0.6905 - val_accuracy: 0.4886\n",
      "Epoch 2/15\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 0.6878 - accuracy: 0.5028 - val_loss: 0.6842 - val_accuracy: 0.4886\n",
      "Epoch 3/15\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 0.6793 - accuracy: 0.5028 - val_loss: 0.6733 - val_accuracy: 0.4886\n",
      "Epoch 4/15\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 0.6654 - accuracy: 0.5028 - val_loss: 0.6569 - val_accuracy: 0.4886\n",
      "Epoch 5/15\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 0.6453 - accuracy: 0.5058 - val_loss: 0.6351 - val_accuracy: 0.5048\n",
      "Epoch 6/15\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 0.6191 - accuracy: 0.5577 - val_loss: 0.6086 - val_accuracy: 0.5770\n",
      "Epoch 7/15\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.5878 - accuracy: 0.6397 - val_loss: 0.5791 - val_accuracy: 0.6458\n",
      "Epoch 8/15\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 0.5536 - accuracy: 0.7048 - val_loss: 0.5488 - val_accuracy: 0.6874\n",
      "Epoch 9/15\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.5187 - accuracy: 0.7489 - val_loss: 0.5198 - val_accuracy: 0.7252\n",
      "Epoch 10/15\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 0.4852 - accuracy: 0.7749 - val_loss: 0.4937 - val_accuracy: 0.7462\n",
      "Epoch 11/15\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.4545 - accuracy: 0.7958 - val_loss: 0.4711 - val_accuracy: 0.7616\n",
      "Epoch 12/15\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 0.4271 - accuracy: 0.8138 - val_loss: 0.4521 - val_accuracy: 0.7722\n",
      "Epoch 13/15\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 0.4029 - accuracy: 0.8260 - val_loss: 0.4363 - val_accuracy: 0.7806\n",
      "Epoch 14/15\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 0.3816 - accuracy: 0.8374 - val_loss: 0.4233 - val_accuracy: 0.7918\n",
      "Epoch 15/15\n",
      "20/20 [==============================] - 1s 44ms/step - loss: 0.3627 - accuracy: 0.8468 - val_loss: 0.4125 - val_accuracy: 0.7992\n",
      "CPU times: user 15.9 s, sys: 568 ms, total: 16.4 s\n",
      "Wall time: 21.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe5675f22f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wYnVedSPfmX"
   },
   "source": [
    "With this approach the model reaches a validation accuracy of around 85% \n",
    "\n",
    "Note: Your results may be a bit different, depending on how weights were randomly initialized before training the embedding layer. \n",
    "\n",
    "You can look into the model summary to learn more about each layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "mDCgjWyq_0dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_input (Sequential)     (None, 100, 16)           160000    \n",
      "                                                                 \n",
      " classifier_head (Sequential  (None, 1)                289       \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiQbOJZ2WBFY"
   },
   "source": [
    "Visualize the model metrics in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "_Uanp2YH8RzU"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-62f43c23543a1bfb\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-62f43c23543a1bfb\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# docs_infra: no_execute\n",
    "%load_ext tensorboard\n",
    "# %tensorboard --logdir logs\n",
    "%tensorboard --logdir /home/rob/Data/Documents/Github/rkaunismaa/logs/TensorFlowTutorials/text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKp2PvLYI-r2"
   },
   "source": [
    "## Vocabulary remapping\n",
    "\n",
    "Now you're going to update the vocabulary and continue with warm-started training.\n",
    "\n",
    "First, get the base vocabulary and embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "HFgt2n6HJDAw"
   },
   "outputs": [],
   "source": [
    "embedding_weights_base = (\n",
    "    model.get_layer(\"text_input\").get_layer(\"embedding\").get_weights()[0]\n",
    ")\n",
    "vocab_base = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8wuaIVkJaNw"
   },
   "source": [
    "Define a new vectorization layer to generate a new bigger vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "_-YcdW4XJlcX"
   },
   "outputs": [],
   "source": [
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size_new = 10200\n",
    "sequence_length = 100\n",
    "\n",
    "vectorize_layer_new = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size_new,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer_new.adapt(text_ds)\n",
    "\n",
    "# Get the new vocabulary\n",
    "vocab_new = vectorize_layer_new.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "lny782PFNF3j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bullying',\n",
       " 'bumps',\n",
       " 'canvas',\n",
       " 'carole',\n",
       " 'chains',\n",
       " 'chairman',\n",
       " 'checks',\n",
       " 'coarse',\n",
       " 'competitive',\n",
       " 'component',\n",
       " 'compound',\n",
       " 'confirm',\n",
       " 'contemplate',\n",
       " 'coping',\n",
       " 'corporations',\n",
       " 'costuming',\n",
       " 'counterpart',\n",
       " 'crop',\n",
       " 'custody',\n",
       " 'cyborgs',\n",
       " 'daft',\n",
       " 'danced',\n",
       " 'daphne',\n",
       " 'darkest',\n",
       " 'davids',\n",
       " 'december',\n",
       " 'declared',\n",
       " 'defence',\n",
       " 'delve',\n",
       " 'demonstration',\n",
       " 'dense',\n",
       " 'denver',\n",
       " 'devilish',\n",
       " 'devious',\n",
       " 'dickinson',\n",
       " 'digs',\n",
       " 'directorwriter',\n",
       " 'download',\n",
       " 'effortless',\n",
       " 'electricity',\n",
       " 'elliot',\n",
       " 'enlightenment',\n",
       " 'erratic',\n",
       " 'exceedingly',\n",
       " 'eyeballs',\n",
       " 'fearless',\n",
       " 'fenton',\n",
       " 'fiennes',\n",
       " 'filter',\n",
       " 'fireworks',\n",
       " 'flipping',\n",
       " 'float',\n",
       " 'foggy',\n",
       " 'forgivable',\n",
       " 'framework',\n",
       " 'fulllength',\n",
       " 'funds',\n",
       " 'gamut',\n",
       " 'geeks',\n",
       " 'glee',\n",
       " 'goo',\n",
       " 'gripe',\n",
       " 'hardest',\n",
       " 'harmony',\n",
       " 'henchman',\n",
       " 'heritage',\n",
       " 'hg',\n",
       " 'hi',\n",
       " 'hightech',\n",
       " 'homework',\n",
       " 'houston',\n",
       " 'howards',\n",
       " 'hunger',\n",
       " 'imho',\n",
       " 'immigrants',\n",
       " 'improvised',\n",
       " 'impulse',\n",
       " 'inch',\n",
       " 'interpret',\n",
       " 'intimidating',\n",
       " 'iowa',\n",
       " 'jaffar',\n",
       " 'jeep',\n",
       " 'jock',\n",
       " 'kriemhild',\n",
       " 'kristofferson',\n",
       " 'lassie',\n",
       " 'laughoutloud',\n",
       " 'lennon',\n",
       " 'librarian',\n",
       " 'liza',\n",
       " 'locker',\n",
       " 'lommel',\n",
       " 'loren',\n",
       " 'lowered',\n",
       " 'marital',\n",
       " 'martins',\n",
       " 'mastroianni',\n",
       " 'megan',\n",
       " 'melt',\n",
       " 'mischievous',\n",
       " 'monstrosity',\n",
       " 'monumental',\n",
       " 'morse',\n",
       " 'mostel',\n",
       " 'muddy',\n",
       " 'noah',\n",
       " 'noirs',\n",
       " 'nostril',\n",
       " 'numbing',\n",
       " 'occupation',\n",
       " 'oceans',\n",
       " 'onesided',\n",
       " 'opus',\n",
       " 'organ',\n",
       " 'osullivan',\n",
       " 'otoole',\n",
       " 'overnight',\n",
       " 'parisian',\n",
       " 'partial',\n",
       " 'patriotism',\n",
       " 'pbs',\n",
       " 'penchant',\n",
       " 'penguin',\n",
       " 'plotted',\n",
       " 'powerfully',\n",
       " 'pows',\n",
       " 'practicing',\n",
       " 'prehistoric',\n",
       " 'prestigious',\n",
       " 'prevalent',\n",
       " 'prevents',\n",
       " 'profits',\n",
       " 'promotion',\n",
       " 'puke',\n",
       " 'pulse',\n",
       " 'punchline',\n",
       " 'quarters',\n",
       " 'rainer',\n",
       " 'ranting',\n",
       " 'rapists',\n",
       " 'rapture',\n",
       " 'rarity',\n",
       " 'rays',\n",
       " 'recommending',\n",
       " 'redeemed',\n",
       " 'refuge',\n",
       " 'refugee',\n",
       " 'relates',\n",
       " 'religions',\n",
       " 'remaking',\n",
       " 'renee',\n",
       " 'reply',\n",
       " 'restoration',\n",
       " 'resurrection',\n",
       " 'retreat',\n",
       " 'retro',\n",
       " 'rockets',\n",
       " 'romano',\n",
       " 'rooker',\n",
       " 'rooted',\n",
       " 'runtime',\n",
       " 'sap',\n",
       " 'scarred',\n",
       " 'secluded',\n",
       " 'selfabsorbed',\n",
       " 'separation',\n",
       " 'shattered',\n",
       " 'shenanigans',\n",
       " 'shootings',\n",
       " 'shue',\n",
       " 'silk',\n",
       " 'sm',\n",
       " 'soooo',\n",
       " 'spoton',\n",
       " 'sr',\n",
       " 'staple',\n",
       " 'stepfather',\n",
       " 'stoic',\n",
       " 'stud',\n",
       " 'suite',\n",
       " 'swanson',\n",
       " 'sweetness',\n",
       " 'sybil',\n",
       " 'tease',\n",
       " 'technological',\n",
       " 'tensions',\n",
       " 'theft',\n",
       " 'therapist',\n",
       " 'threats',\n",
       " 'tin',\n",
       " 'towel',\n",
       " 'transform',\n",
       " 'travelling',\n",
       " 'troupe',\n",
       " 'unremarkable',\n",
       " 'unsatisfied',\n",
       " 'untrue',\n",
       " 'vertigo',\n",
       " 'vic'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the new vocabulary tokens that weren't in `vocab_base`\n",
    "set(vocab_base) ^ set(vocab_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHsDOlAnJrFH"
   },
   "source": [
    "Generate updated embeddings using the `keras.utils.warmstart_embedding_matrix` util."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "MgBlw3VnKrBL"
   },
   "outputs": [],
   "source": [
    "# Generate the updated embedding matrix\n",
    "updated_embedding = tf.keras.utils.warmstart_embedding_matrix(\n",
    "    base_vocabulary=vocab_base,\n",
    "    new_vocabulary=vocab_new,\n",
    "    base_embeddings=embedding_weights_base,\n",
    "    new_embeddings_initializer=\"uniform\",\n",
    ")\n",
    "# Update the model variable\n",
    "updated_embedding_variable = tf.Variable(updated_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEhm8fldKyR_"
   },
   "source": [
    "**OR**\n",
    "\n",
    "If you have an embedding matrix which you would like to initialize the new embedding matrix with, use `keras.initializers.Constant` as new_embeddings initializer. Copy the following block to a code cell to try this out.\n",
    "This would be helpful when you have a better embedding matrix initialization for new words in vocab.\n",
    "```\n",
    "# generate updated embedding matrix\n",
    "new_embedding = np.random.rand(len(vocab_new), 16)\n",
    "updated_embedding = tf.keras.utils.warmstart_embedding_matrix(\n",
    "            base_vocabulary=vocab_base,\n",
    "            new_vocabulary=vocab_new,\n",
    "            base_embeddings=embedding_weights_base,\n",
    "            new_embeddings_initializer=tf.keras.initializers.Constant(\n",
    "                new_embedding\n",
    "            )\n",
    "        )\n",
    "# update model variable\n",
    "updated_embedding_variable = tf.Variable(updated_embedding)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbKLjXfhLUVa"
   },
   "source": [
    "Verify if the embedding matrix's shape has changed to reflect the new vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "tYDrBBEtLWZQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10200, 16])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_embedding_variable.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCd8LnSILZqk"
   },
   "source": [
    "Now that you have the updated embedding matrix, the next step is to update the layer weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "i-PdukkBLlx1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_input_new\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (TextV  (None, 100)              0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 100, 16)           163200    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 163,200\n",
      "Trainable params: 163,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10200, 16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embedding_layer_new = Embedding(\n",
    "    vectorize_layer_new.vocabulary_size(), embedding_dim, name=\"embedding\"\n",
    ")\n",
    "text_embedding_layer_new.build(input_shape=[None])\n",
    "text_embedding_layer_new.embeddings.assign(updated_embedding)\n",
    "text_input_new = tf.keras.Sequential(\n",
    "    [vectorize_layer_new, text_embedding_layer_new], name=\"text_input_new\"\n",
    ")\n",
    "text_input_new.summary()\n",
    "\n",
    "# Verify the shape of updated weights\n",
    "# The new weights shape should reflect the new vocabulary size\n",
    "text_input_new.get_layer(\"embedding\").get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juAdUZkVMpEj"
   },
   "source": [
    "Modify the model architecture to use the new text vectorization layer.\n",
    "\n",
    "You can also load the model from a checkpoint and update the model architecture as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "etCo20sPNn2C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_input_new (Sequential)  (None, 100, 16)          163200    \n",
      "                                                                 \n",
      " classifier_head (Sequential  (None, 1)                289       \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 163,489\n",
      "Trainable params: 163,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "warm_started_model = tf.keras.Sequential([text_input_new, classifier_head])\n",
    "warm_started_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hi4r5FubN202"
   },
   "source": [
    "You have successfully updated the model to accept a new vocabulary. The embedding layer is updated to map old vocabulary words to old embeddings and initialize embeddings for new vocabulary words to be learnt. The learned weights of the rest of the model will remain the same. The model is warm-started to continue to train from where it left off previously.\n",
    "\n",
    "You can now verify that the remapping worked. Get index of the vocabulary word \"the\" that is present both in base and new vocabulary and compare the embedding values. They should be equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "vCdlWvpPPEow"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True], shape=(16,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "# New vocab words\n",
    "base_vocab_index = vectorize_layer(\"the\")[0]\n",
    "new_vocab_index = vectorize_layer_new(\"the\")[0]\n",
    "print(\n",
    "    warm_started_model.get_layer(\"text_input_new\").get_layer(\"embedding\")(\n",
    "        new_vocab_index\n",
    "    )\n",
    "    == embedding_weights_base[base_vocab_index]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1xX0XCEReRC"
   },
   "source": [
    "## Continue with warm-started training\n",
    "\n",
    "Notice how the training is warm-started. The accuracy of first epoch is around 85%. Close to the accuracy where the previous traning ended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "OtbXMQsTRdvq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "20/20 [==============================] - 4s 142ms/step - loss: 0.3477 - accuracy: 0.8515 - val_loss: 0.4045 - val_accuracy: 0.8040\n",
      "Epoch 2/15\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 0.3333 - accuracy: 0.8594 - val_loss: 0.3978 - val_accuracy: 0.8060\n",
      "Epoch 3/15\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 0.3208 - accuracy: 0.8635 - val_loss: 0.3922 - val_accuracy: 0.8072\n",
      "Epoch 4/15\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 0.3094 - accuracy: 0.8679 - val_loss: 0.3877 - val_accuracy: 0.8116\n",
      "Epoch 5/15\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.2988 - accuracy: 0.8745 - val_loss: 0.3841 - val_accuracy: 0.8144\n",
      "Epoch 6/15\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.2889 - accuracy: 0.8789 - val_loss: 0.3813 - val_accuracy: 0.8166\n",
      "Epoch 7/15\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.2796 - accuracy: 0.8834 - val_loss: 0.3792 - val_accuracy: 0.8180\n",
      "Epoch 8/15\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 0.2709 - accuracy: 0.8869 - val_loss: 0.3778 - val_accuracy: 0.8202\n",
      "Epoch 9/15\n",
      "20/20 [==============================] - 1s 52ms/step - loss: 0.2627 - accuracy: 0.8910 - val_loss: 0.3770 - val_accuracy: 0.8218\n",
      "Epoch 10/15\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.2548 - accuracy: 0.8946 - val_loss: 0.3767 - val_accuracy: 0.8228\n",
      "Epoch 11/15\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 0.2474 - accuracy: 0.8979 - val_loss: 0.3769 - val_accuracy: 0.8242\n",
      "Epoch 12/15\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 0.2403 - accuracy: 0.9011 - val_loss: 0.3777 - val_accuracy: 0.8246\n",
      "Epoch 13/15\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 0.2336 - accuracy: 0.9050 - val_loss: 0.3788 - val_accuracy: 0.8240\n",
      "Epoch 14/15\n",
      "20/20 [==============================] - 1s 43ms/step - loss: 0.2270 - accuracy: 0.9079 - val_loss: 0.3803 - val_accuracy: 0.8230\n",
      "Epoch 15/15\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 0.2208 - accuracy: 0.9108 - val_loss: 0.3822 - val_accuracy: 0.8236\n",
      "CPU times: user 15.1 s, sys: 257 ms, total: 15.4 s\n",
      "Wall time: 21.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe5671fa0b0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5z67BhOZR6do"
   },
   "source": [
    "## Visualize warm-started training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "eXPXUfw3QTY-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 223907), started 0:00:23 ago. (Use '!kill 223907' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e325c34fead5f572\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e325c34fead5f572\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# docs_infra: no_execute\n",
    "%reload_ext tensorboard\n",
    "# %tensorboard --logdir logs\n",
    "%tensorboard --logdir /home/rob/Data/Documents/Github/rkaunismaa/logs/TensorFlowTutorials/text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-9MOqehCQa8"
   },
   "source": [
    "<!-- <img class=\"tfo-display-only-on-site\" src=\"https://tensorflow.org/tutorials/text/images/tensorboard-2.png\"/> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SrmuEgJQIIP"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "In this tutorial you learned how to:\n",
    "\n",
    "* Train a sentiment classification model from scratch on a small vocabulary dataset.\n",
    "* Update the model architecture and warm start the embedding matrix when the vocabulary size changes.\n",
    "* Continuously improve model accuracy with expanding datasets\n",
    "\n",
    "To learn more about embeddings check out the [Word2Vec](https://www.tensorflow.org/tutorials/text/word2vec) and [Transformer model for language understanding](https://www.tensorflow.org/text/tutorials/transformer) tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Run Date: Thursday, January 12, 2023\n",
      "# Run Time: 00:01:06\n"
     ]
    }
   ],
   "source": [
    "endTime = time.time()\n",
    "\n",
    "elapsedTime = time.strftime(\"%H:%M:%S\", time.gmtime(endTime - startTime))\n",
    "\n",
    "print(todaysDate.strftime('# Run Date: %A, %B %d, %Y'))\n",
    "print(f\"# Run Time: {elapsedTime}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "warmstart_embedding_matrix.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "fdc7e056a9505d324a6e72b598bbff601d5b3681c979b699a70a4870ab6885c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
